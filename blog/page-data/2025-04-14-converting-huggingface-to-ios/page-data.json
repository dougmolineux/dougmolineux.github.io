{"componentChunkName":"component---src-templates-blog-post-js","path":"/2025-04-14-converting-huggingface-to-ios/","result":{"data":{"markdownRemark":{"html":"<p>Converting Hugging Face model to work with iOS</p>\n<p>I wanted to be able to utilize a hugging face model running locally on an iOS device. I have read that it's possible to do this, but after a few hours of of attempting to do this, it turns out that it's actually quite difficult.</p>\n<p>I wanted to use this model here:</p>\n<p>Falconsai/text_summarization</p>\n<p>Google Gemini 2.5 pro said this about my attempt:</p>\n<pre><code class=\"language-Okay,\" data-meta=\"converting a Hugging Face model like Falconsai/text_summarization (which is based on Pegasus) for use on an iPhone involves converting it to the Core ML format. Doing this in Google Colab is a good approach.\">\nSequence-to-sequence models like Pegasus can be more complex to convert than other types because they have distinct encoder and decoder components. Hugging Face's optimum library is often the best tool for this, as it's designed to handle these conversions.\n\nHere's a Python script you can run in a Google Colab notebook. It includes installing dependencies, loading the model, attempting the conversion using optimum, and preparing the resulting Core ML package for download.\n</code></pre>\n<p>Encountered endless dependency errors and I've been trying to keep everything inside Google because it seemed like Google has the best tools for Python, especially with its Google Colab environment.</p>\n<p>Perhaps it's the model that I'm trying to use that's the problem or perhaps it's just using Python dependencies inside Google Google Colab.</p>","frontmatter":{"title":"Converting Huggingface Model to work on iOS","date":"2025-04-14"}}},"pageContext":{"slug":"/2025-04-14-converting-huggingface-to-ios/"}},"staticQueryHashes":[],"slicesMap":{}}