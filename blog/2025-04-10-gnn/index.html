<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="generator" content="Gatsby 5.14.1"/><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){const t=e.target;if(void 0===t.dataset.mainImage)return;if(void 0===t.dataset.gatsbyImageSsr)return;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div style="max-width:1200px;margin:0 auto;padding:20px;font-family:Arial, sans-serif;line-height:1.6;color:#4A4A4A;background-color:#F9F9F9"><style>
          h1, h2, h3, h4, h5, h6 {
            margin-top: 1.5em;
            margin-bottom: 0.5em;
            color: #2C5D63; // Soft teal for headers
          }

          h1 {
            font-size: 2.5rem;
          }

          h2 {
            font-size: 2rem;
          }

          h3 {
            font-size: 1.75rem;
          }

          p {
            margin-bottom: 1.5em;
          }

          a {
            color: #2C5D63; // Soft teal for links
            text-decoration: none;
          }

          a:hover {
            text-decoration: underline;
            color: #3A7A7A; // Slightly darker teal on hover
          }

          ul, ol {
            margin-bottom: 1.5em;
            padding-left: 20px;
          }

          li {
            margin-bottom: 0.5em;
          }

          code {
            background: #E0F2F1; // Very light teal for code background
            padding: 2px 5px;
            border-radius: 3px;
            font-family: &quot;Courier New&quot;, monospace;
            font-size: 0.9em;
            color: #2C5D63; // Soft teal for code text
          }

          pre {
            background: #E0F2F1; // Very light teal for code block background
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin-bottom: 1.5em;
            color: #2C5D63; // Soft teal for code block text
          }

          blockquote {
            border-left: 4px solid #A8DADC; // Light teal for blockquote border
            padding-left: 15px;
            margin: 1.5em 0;
            color: #4A4A4A; // Soft dark gray for blockquote text
            font-style: italic;
          }

          table {
            width: &quot;100%&quot;,
            border-collapse: &quot;collapse&quot;,
            margin-bottom: &quot;1.5em&quot;,
          }

          th, td {
            padding: &quot;10px&quot;,
            border: &quot;1px solid #A8DADC&quot;, // Light teal for table borders
            text-align: &quot;left&quot;,
          }

          th {
            background: &quot;#E0F2F1&quot;, // Very light teal for table header background
          }
        </style><header style="border-bottom:2px solid #A8DADC;padding-bottom:10px;margin-bottom:20px"><h1 style="margin:0;color:#2C5D63">doug.molineux.blog</h1></header><a href="../">Blog</a><main><article><h1 style="font-size:2.5rem;margin-bottom:10px;color:#007acc">Graph Neural Networks (GNNs)</h1><p style="color:#777;font-size:0.9rem;margin-bottom:30px">4/9/2025</p><div style="font-size:1.1rem;line-height:1.8"><h1>Graph Neural Networks (GNNs): Revolutionizing AI for Molecular Structures</h1>
<p>Imagine trying to teach a computer to understand a molecule. You could describe it as a collection of atoms with specific properties, but that misses something fundamental: the connections between those atoms. This network of connections—this graph—is precisely what makes molecules behave the way they do. <strong>Graph Neural Networks (GNNs)</strong> are a family of deep learning architectures specifically designed to process data that can be represented as graphs, making them uniquely suited for understanding molecular structures and revolutionizing modern drug discovery.</p>
<h2>What Are Graph Neural Networks?</h2>
<p>At their simplest, GNNs are neural networks that can directly operate on graph-structured data. Unlike traditional neural networks that expect data in regular formats like grids (images) or sequences (text), GNNs work with interconnected nodes and edges. In a molecular context, atoms become nodes and chemical bonds become edges.</p>
<p>Traditional machine learning approaches to molecular analysis often relied on manually engineered "fingerprints" or descriptors that attempted to capture molecular properties. These methods essentially flattened the rich structural information into vectors, losing critical spatial and connectivity information in the process. GNNs, by contrast, preserve and leverage this structural information directly.</p>
<p>The core insight behind GNNs is a process called <strong>message passing</strong>. In this paradigm, each node (atom) in the graph iteratively gathers information from its neighbors, updates its own representation based on this information, and then passes on its updated knowledge. After several rounds of message passing, each node's representation contains information not just about itself, but about its local neighborhood and, ultimately, the entire graph structure.</p>
<h2>How GNNs Work: The Technical Details</h2>
<p>While the concept is elegant, the implementation involves several sophisticated components working together:</p>
<ol>
<li>
<p><strong>Node Feature Initialization</strong>: Each atom in a molecule starts with a set of features that might include atomic number, hybridization state, formal charge, and more. These features form the initial representation of each node.</p>
</li>
<li>
<p><strong>Edge Feature Encoding</strong>: Similarly, bonds between atoms have properties like bond type (single, double, triple), conjugation, and stereochemistry that form edge features.</p>
</li>
<li>
<p><strong>Message Passing Layers</strong>: The heart of a GNN architecture consists of these specialized layers where each node aggregates information from its neighbors. Mathematically, this often looks like:</p>
<p>h_v^(k+1) = Update(h_v^k, Aggregate({h_u^k : u ∈ N(v)}))</p>
<p>Where h_v^k is the feature vector for node v at iteration k, N(v) is the neighborhood of v, and Aggregate and Update are learnable functions.</p>
</li>
<li>
<p><strong>Readout Function</strong>: After several iterations of message passing, a readout function combines node-level representations into a single graph-level representation, which can then be used for prediction tasks.</p>
</li>
</ol>
<p>The architecture can be trained end-to-end using standard deep learning techniques, typically supervised learning with backpropagation.</p>
<h2>Popular GNN Architectures</h2>
<p>The field has rapidly evolved with several specialized architectures:</p>
<p><strong>Graph Convolutional Networks (GCNs)</strong> pioneered by <a href="https://arxiv.org/abs/1609.02907">Kipf and Welling</a> provide a spectral approach to graph convolutions, offering an efficient approximation that scales to large graphs.</p>
<p><strong>Graph Attention Networks (GATs)</strong> introduced by <a href="https://arxiv.org/abs/1710.10903">Veličković et al.</a> incorporate attention mechanisms that allow nodes to weigh the importance of different neighbors, providing more flexibility in message passing.</p>
<p><strong>Message Passing Neural Networks (MPNNs)</strong> formalized by <a href="https://arxiv.org/abs/1704.01212">Gilmer et al.</a> at Google provide a general framework encompassing many GNN variants, with special focus on quantum chemistry applications.</p>
<p><strong>Directional Message Passing Neural Networks (D-MPNNs)</strong> extend the message passing paradigm by incorporating bond directions, which is particularly important for capturing stereochemistry in drug-like molecules.</p>
<h2>GNNs in Drug Discovery: A Perfect Match</h2>
<p>The application of GNNs to drug discovery feels almost inevitable in retrospect. Molecules are naturally represented as graphs, and many of the properties we care about in drug development—binding affinity, solubility, toxicity—are fundamentally determined by molecular structure.</p>
<h3>Property Prediction</h3>
<p>One of the most direct applications of GNNs is predicting molecular properties. Traditional approaches like Quantitative Structure-Activity Relationship (QSAR) modeling have been used for decades, but GNNs have demonstrated superior performance across numerous benchmarks.</p>
<p>For example, <a href="https://arxiv.org/abs/1603.00856">Kearnes et al.</a> demonstrated that graph convolutional networks can outperform traditional fingerprint-based methods for predicting solubility, toxicity, and other pharmacologically relevant properties. More recently, the <a href="https://tdcommons.ai/">Therapeutics Data Commons</a> has established benchmarks where GNN-based approaches consistently rank at the top.</p>
<h3>Virtual Screening</h3>
<p><strong>Virtual screening</strong> involves computationally evaluating large libraries of compounds to identify those most likely to bind to a target protein. GNNs have been particularly successful here, as demonstrated by DeepChem's implementation of GNN-based virtual screening tools that are now used by numerous pharmaceutical companies.</p>
<p>A fascinating example comes from <a href="https://www.cell.com/cell/fulltext/S0092-8674(20)30102-1">Stokes et al.</a> who used a GNN to identify a novel antibiotic (halicin) with activity against bacteria that had developed resistance to conventional antibiotics. The model was trained on a dataset of known antibiotics and their activity against various bacterial strains, then used to screen a library of over 100 million molecules.</p>
<h3>De Novo Molecular Design</h3>
<p>Perhaps the most ambitious application is using GNNs not just to evaluate existing molecules, but to create entirely new ones with desired properties. This typically involves combining GNNs with generative models.</p>
<p>One approach, demonstrated by <a href="https://arxiv.org/abs/1812.01070">Jin et al.</a> in their Junction Tree Variational Autoencoder, uses a GNN to encode molecular graphs into a latent space, and then decodes points from this space back into valid molecules. By navigating this latent space, researchers can generate molecules with specific desired properties.</p>
<p>Another approach combines GNNs with reinforcement learning, where the model is rewarded for generating molecules with desired properties. <a href="https://arxiv.org/abs/1806.02473">You et al.</a> demonstrated this with their Graph Convolutional Policy Network, which learned to build molecules atom-by-atom while optimizing for specific properties.</p>
<h2>Real-World Impact and Industry Adoption</h2>
<p>The pharmaceutical industry has enthusiastically embraced GNNs, with numerous startups and established companies integrating them into their discovery pipelines:</p>
<p><strong>Relay Therapeutics</strong> uses GNNs as part of their platform to understand protein motion and its implications for drug binding, helping them discover novel cancer therapeutics.</p>
<p><strong>Atomwise</strong> pioneered the use of deep learning for virtual screening and has screened billions of compounds across hundreds of protein targets, with several candidates now in preclinical development.</p>
<p><strong>Insilico Medicine</strong> combines GNNs with generative adversarial networks in their Chemistry42 platform, which has produced candidates for challenging targets like DDR1 kinase in record time.</p>
<p><strong>Exscientia</strong> made history with the first AI-designed drug to enter clinical trials, using graph-based approaches as part of their pharmacology-driven AI platform.</p>
<h2>Challenges and Future Directions</h2>
<p>Despite their impressive success, GNNs in drug discovery face several challenges:</p>
<p><strong>Limited training data</strong> remains a significant obstacle. While public datasets like ChEMBL contain millions of data points, they are sparse across the vast chemical space and often noisy or inconsistent.</p>
<p><strong>Interpretability</strong> is crucial in the pharmaceutical context where understanding the "why" behind predictions can guide medicinal chemistry optimization. Efforts to make GNNs more interpretable include attention visualization and attribution methods.</p>
<p><strong>Multi-scale integration</strong> represents the frontier of GNN research in drug discovery. Molecules don't exist in isolation but interact with proteins, which themselves exist within cellular contexts. Developing GNNs that can seamlessly traverse these scales is an active area of research.</p>
<p><strong>3D structures</strong> contain critical information about molecular conformations that 2D graph representations might miss. Recent work on geometric deep learning and 3D-aware GNNs, such as <a href="https://arxiv.org/abs/1706.08566">SchNet</a> and <a href="https://arxiv.org/abs/2003.03123">DimeNet</a>, aims to address this by incorporating distance and angular information.</p>
<h2>Getting Started with GNNs for Drug Discovery</h2>
<p>For software engineers interested in exploring this field, several excellent resources and frameworks exist:</p>
<p><strong>PyTorch Geometric</strong> provides an extensive collection of GNN implementations and utilities specifically designed for working with graph data, including chemistry-specific features.</p>
<p><strong>DeepChem</strong>, an open-source platform for deep learning in drug discovery, offers pre-built GNN architectures and molecular featurization tools, along with comprehensive tutorials.</p>
<p><strong>RDKit</strong> serves as the backbone for molecular manipulation and representation in most GNN-based drug discovery pipelines.</p>
<p><strong>TorchDrug</strong> focuses specifically on drug discovery applications of deep learning, with built-in support for various GNN architectures.</p>
<p>A typical workflow might involve:</p>
<ol>
<li>Preparing molecular data using RDKit</li>
<li>Featurizing molecules as graphs with atom and bond features</li>
<li>Implementing or selecting a GNN architecture</li>
<li>Training the model on a property prediction task</li>
<li>Evaluating performance against traditional methods</li>
</ol>
<p>For those without extensive chemistry background, starting with property prediction tasks using datasets from MoleculeNet is a good entry point.</p>
<h2>Conclusion</h2>
<p>Graph Neural Networks represent a paradigm shift in how we approach computational drug discovery. By preserving and leveraging the inherent graph structure of molecules, they enable more accurate predictions, more efficient screening, and more creative design of potential therapeutics.</p>
<p>The synergy between GNNs and drug discovery is particularly powerful because it combines rigorous chemical knowledge with modern deep learning approaches. Rather than replacing domain expertise, GNNs amplify it by automating the exploration of chemical space and providing insights that might otherwise remain hidden.</p>
<p>As we look to the future, the integration of GNNs with other emerging technologies—federated learning for privacy-preserving collaboration, active learning for experiment design, and quantum computing for enhanced simulation—promises to further accelerate the pace of discovery.</p>
<p>For software engineers entering this space, the opportunity to impact human health through the application of cutting-edge AI is profound. The molecules designed today with the help of GNNs could become the life-saving treatments of tomorrow, making this field not just intellectually fascinating but deeply meaningful.</p>
<h2>References</h2>
<ol>
<li>
<p>Kipf, T. N., &#x26; Welling, M. (2016). Semi-supervised classification with graph convolutional networks. <a href="https://arxiv.org/abs/1609.02907">arXiv:1609.02907</a>.</p>
</li>
<li>
<p>Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., &#x26; Bengio, Y. (2017). Graph attention networks. <a href="https://arxiv.org/abs/1710.10903">arXiv:1710.10903</a>.</p>
</li>
<li>
<p>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., &#x26; Dahl, G. E. (2017). Neural message passing for quantum chemistry. <a href="https://arxiv.org/abs/1704.01212">arXiv:1704.01212</a>.</p>
</li>
<li>
<p>Kearnes, S., McCloskey, K., Berndl, M., Pande, V., &#x26; Riley, P. (2016). Molecular graph convolutions: moving beyond fingerprints. <a href="https://arxiv.org/abs/1603.00856">arXiv:1603.00856</a>.</p>
</li>
<li>
<p>Stokes, J. M., Yang, K., Swanson, K., Jin, W., Cubillos-Ruiz, A., Donghia, N. M., ... &#x26; Collins, J. J. (2020). A deep learning approach to antibiotic discovery. Cell, 180(4), 688-702. <a href="https://www.cell.com/cell/fulltext/S0092-8674(20)30102-1">https://www.cell.com/cell/fulltext/S0092-8674(20)30102-1</a></p>
</li>
<li>
<p>Jin, W., Barzilay, R., &#x26; Jaakkola, T. (2018). Junction tree variational autoencoder for molecular graph generation. <a href="https://arxiv.org/abs/1812.01070">arXiv:1812.01070</a>.</p>
</li>
<li>
<p>You, J., Liu, B., Ying, R., Pande, V., &#x26; Leskovec, J. (2018). Graph convolutional policy network for goal-directed molecular graph generation. <a href="https://arxiv.org/abs/1806.02473">arXiv:1806.02473</a>.</p>
</li>
<li>
<p>Schütt, K. T., Kindermans, P. J., Sauceda, H. E., Chmiela, S., Tkatchenko, A., &#x26; Müller, K. R. (2017). SchNet: A continuous-filter convolutional neural network for modeling quantum interactions. <a href="https://arxiv.org/abs/1706.08566">arXiv:1706.08566</a>.</p>
</li>
<li>
<p>Klicpera, J., Groß, J., &#x26; Günnemann, S. (2020). Directional message passing for molecular graphs. <a href="https://arxiv.org/abs/2003.03123">arXiv:2003.03123</a>.</p>
</li>
</ol></div></article></main><footer style="margin-top:40px;padding-top:20px;border-top:1px solid #A8DADC;text-align:center;color:#777">© <!-- -->2025<!-- --> doug.molineux.blog. Built with Gatsby.</footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/2025-04-10-gnn/";/*]]>*/</script><!-- slice-start id="_gatsby-scripts-1" -->
          <script
            id="gatsby-chunk-mapping"
          >
            window.___chunkMapping="{\"app\":[\"/app-3606d69e6882b8247df1.js\"],\"component---src-pages-404-js\":[\"/component---src-pages-404-js-3ea0307e216f7661015a.js\"],\"component---src-pages-index-js\":[\"/component---src-pages-index-js-d30f25ae874ed21a5d15.js\"],\"component---src-pages-using-ssr-js\":[\"/component---src-pages-using-ssr-js-145f22713734d613fd81.js\"],\"component---src-templates-blog-post-js\":[\"/component---src-templates-blog-post-js-525ef8cbf38c1fb2525c.js\"]}";
          </script>
        <script>window.___webpackCompilationHash="fa41b22cdec277a2cb6c";</script><script src="/blog/webpack-runtime-fe1e2786b90271c63410.js" async></script><script src="/blog/framework-9fce5d8597f27c4b157b.js" async></script><script src="/blog/app-3606d69e6882b8247df1.js" async></script><!-- slice-end id="_gatsby-scripts-1" --></body></html>